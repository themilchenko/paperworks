\section{Демонстрация механизма асинхронной репликации между кластерами}

В данном разделе рассматривается практическая демонстрация работы
разработанного механизма асинхронной межкластерной репликации. Основной целью
является подтверждение корректности архитектурных решений и реализации
репликации данных между географически или логически разнесёнными кластерами в
условиях модели \textit{active/passive}.

Демонстрация включает в себя описание конфигурации активного и пассивного
кластеров, процесс их запуска, анализ внутреннего состояния узлов и проверку
фактической доставки данных из активного кластера в пассивный. Особое внимание
уделяется воспроизводимости эксперимента и наглядности полученных результатов,
что позволяет напрямую сопоставить теоретические положения, изложенные ранее,
с их практической реализацией.

\subsection{Описание конфигурации активного и пассивного кластеров}

Для демонстрации механизма межкластерной асинхронной репликации были развёрнуты
два независимых кластера, каждый из которых содержит по пять узлов. Оба
кластера имеют одинаковую внутреннюю топологию и различаются только ролью
(\textit{active} / \textit{passive}) и адресным пространством.

Внутреннее устройство конфигурационных файлов полностью соответствует формату,
описанному ранее, однако в рамках демонстрации их параметры имеют отдельное
назначение.

На листинге~\ref{fig:active-config} предствалена конфигурация активного кластера. Отличающими опциями являются поле $cluster\_status$, которое задает стартовое состояние. Не смотря на то, что кластер активный, у него есть список $follow\_list$. Это сделано с расчетом на будущее проектирование механизма отказоустойчивости: если текущий кластер поменяет статус на противоположный, то он знал, к каким адресам ему подключаться для репликации.

\begin{figure}
    \centering
    \includegraphics{assets/active_cluster.png}
    \caption{Конфигурация активного кластера}
    \label{fig:active-config}
\end{figure}

Аналогично, на листинге~\ref{fig:passive-config} представлена конфигурация пассивного кластера. Отличия, как говорилось ранее, в указании адресов для прослушивания, а также в имене и его изначальном статусе.

\begin{figure}
    \centering
    \includegraphics{assets/passive_cluster.png}
    \caption{Конфигурация пассивного кластера}
    \label{fig:passive-config}
\end{figure}

\subsection{Старт кластеров}

На листинге~\ref{lst:start} показан процесс запуска двух кластеров с
использованием утилиты clusterctl. Каждый кластер поднимается на основе
собственного конфигурационного файла, после чего все узлы инициализируют
локальные каталоги, Raft-компоненты и сетевые интерфейсы. Вывод демонстрирует
корректный старт всех узлов и формирование рабочей топологии.

\begin{lstlisting}[
    frame=rlbt,
    caption={Старт кластеров с помощью утилиты clusterctl},
    label=lst:start
]
$ ./bin/clusterctl -c config/cluster1.yml start
2025/11/19 22:09:14 Starting leader "node1"...
2025/11/19 22:09:14 Started "node1" (pid 41617)
2025/11/19 22:09:16 Starting follower "node2"...
2025/11/19 22:09:16 Started "node2" (pid 41626)
2025/11/19 22:09:16 Starting follower "node3"...
2025/11/19 22:09:16 Started "node3" (pid 41627)
2025/11/19 22:09:16 Starting follower "node4"...
2025/11/19 22:09:16 Started "node4" (pid 41628)
2025/11/19 22:09:16 Starting follower "node5"...
2025/11/19 22:09:16 Started "node5" (pid 41629)

$ ./bin/clusterctl -c config/cluster2.yml start
2025/11/19 22:09:21 Starting leader "node1"...
2025/11/19 22:09:21 Started "node1" (pid 41665)
2025/11/19 22:09:23 Starting follower "node2"...
2025/11/19 22:09:23 Started "node2" (pid 41672)
2025/11/19 22:09:23 Starting follower "node3"...
2025/11/19 22:09:23 Started "node3" (pid 41673)
2025/11/19 22:09:23 Starting follower "node4"...
2025/11/19 22:09:23 Started "node4" (pid 41674)
2025/11/19 22:09:23 Starting follower "node5"...
2025/11/19 22:09:23 Started "node5" (pid 41675)
\end{lstlisting}

На листинге~\ref{lst:struct} представлена структура файлов работы кластеров. Каждый узел кластера имеет файл с логами, файл с id процесса, папку с WAL-записями и снапшотами, которые ротируются в процессе времени.

\begin{lstlisting}[
    caption={Структура системной директории кластеров},
    label=lst:struct
]
$ tree var  

var/
  cluster1/
    node1/
      node.log
      node.pid
      snapshots/
      wal/
        00000000000000000001-0000000000000000.wal
        wal-meta.db
    node2/
      node.log
      node.pid
      snapshots/
      wal/
        00000000000000000001-0000000000000000.wal
        wal-meta.db
    node3/
      node.log
      node.pid
      snapshots/
      wal/
        00000000000000000001-0000000000000000.wal
        wal-meta.db
    node4/
      node.log
      node.pid
      snapshots/
      wal/
        00000000000000000001-0000000000000000.wal
        wal-meta.db
    node5/
      node.log
      node.pid
      snapshots/
      wal/
        00000000000000000001-0000000000000000.wal
        wal-meta.db

  cluster2/
    node1/
      node.log
      node.pid
      snapshots/
      wal/
        00000000000000000001-0000000000000000.wal
        wal-meta.db
    node2/
      node.log
      node.pid
      snapshots/
      wal/
        00000000000000000001-0000000000000000.wal
        wal-meta.db
    node3/
      node.log
      node.pid
      snapshots/
      wal/
        00000000000000000001-0000000000000000.wal
        wal-meta.db
    node4/
      node.log
      node.pid
      snapshots/
      wal/
        00000000000000000001-0000000000000000.wal
        wal-meta.db
    node5/
      node.log
      node.pid
      snapshots/
      wal/
        00000000000000000001-0000000000000000.wal
        wal-meta.db
\end{lstlisting}

Для каждого кластера формируется отдельная директория внутри var/, содержащая подпапки для узлов. Каждый узел имеет:

\begin{itemize}
  \item журнал WAL;
  \item каталог снапшотов;
  \item PID-файл;
  \item лог работы.
\end{itemize}

Структура подтверждает корректную инициализацию подсистем WAL и snapshot для всех узлов обоих кластеров.

На листинге~\ref{lst:status} представлен вывод команды \textit{clusterctl status} для активного и пассивного
кластеров. Листинг показывает лидера каждого кластера и список последовательных
узлов (followers), что подтверждает стабильную работу алгоритма Raft и
корректное формирование кворума в обоих кластерах.

\begin{lstlisting}[
    frame=rlbt,
    caption={Статус активного и пассивного кластеров соответственно},
    label=lst:status
]
$ ./bin/clusterctl -c config/cluster1.yml status

Leader:   node1 (127.0.0.1:9000)
Followers:
  - node2 (127.0.0.1:9001)
  - node5 (127.0.0.1:9004)
  - node3 (127.0.0.1:9002)
  - node4 (127.0.0.1:9003)

$ ./bin/clusterctl -c config/cluster2.yml status

Leader:   node1 (127.0.0.1:19000)
Followers:
  - node2 (127.0.0.1:19001)
  - node4 (127.0.0.1:19003)
  - node3 (127.0.0.1:19002)
  - node5 (127.0.0.1:19004)
\end{lstlisting}

На листинге~\ref{lst:fill} демонстрирует практическую работу репликации. Сначала в активный
кластер вставляются десять пар ключ–значение с помощью HTTP-запросов. Затем
выполняются запросы /keys как к активному, так и к пассивному кластеру. Оба
ответа идентичны, что подтверждает успешный механизм межкластерной репликации:
все изменения, произведённые в активном кластере, были корректно доставлены и
применены в пассивном.

\begin{lstlisting}[
    frame=rlbt,
    caption={Заполнение активного кластера и демонстрация репликации на пассивный кластер},
    label=lst:fill
]
$ for i in $(seq 1 10); do
  curl -X POST localhost:8080/key -d "{\"key$i\":\"value$i\"}"
done

$ curl 'localhost:8081/keys?limit=10' | jq
{
  "key1": "value1",
  "key10": "value10",
  "key2": "value2",
  "key3": "value3",
  "key4": "value4",
  "key5": "value5",
  "key6": "value6",
  "key7": "value7",
  "key8": "value8",
  "key9": "value9"
}

$ curl 'localhost:18084/keys?limit=10' | jq
{
  "key1": "value1",
  "key10": "value10",
  "key2": "value2",
  "key3": "value3",
  "key4": "value4",
  "key5": "value5",
  "key6": "value6",
  "key7": "value7",
  "key8": "value8",
  "key9": "value9"
}
\end{lstlisting}

Листинг~\ref{lst:replication} содержит фрагмент внутреннего журнала пассивного лидера. В логе
фиксируются: получение снапшота (если это первый запуск), подключение к
gRPC-потоку, последовательное получение WAL-записей, применение каждого события
к локальному хранилищу, обновление счётчика $last\_applied$.

\begin{lstlisting}[
    frame=rlbt,
    caption={Лог репликации мастера пассивного кластера},
    label=lst:replication
]
2025-11-19 22:47:30.098628 I | [replica-pool] trying 127.0.0.1:9091
2025-11-19 22:47:30.100131 I | [replica-pool] 127.0.0.1:9091 not master, switching...
2025-11-19 22:47:30.100163 I | [replica-pool] trying 127.0.0.1:9092
2025-11-19 22:47:30.102160 I | [replica-pool] 127.0.0.1:9092 not master, switching...
2025-11-19 22:47:30.102191 I | [replica-pool] trying 127.0.0.1:9093
2025-11-19 22:47:30.104218 I | [replica-pool] 127.0.0.1:9093 not master, switching...
2025-11-19 22:47:30.104240 I | [replica-pool] trying 127.0.0.1:9094
2025-11-19 22:47:30.106313 I | [replica-pool] 127.0.0.1:9094 not master, switching...
2025-11-19 22:47:30.106338 I | [replica-pool] trying 127.0.0.1:9090
2025-11-19 22:53:52.911278 I | [replica] event 7 (42 bytes)
2025-11-19 22:53:53.411039 I | [replica] event 8 (42 bytes)
2025-11-19 22:53:53.420613 I | [replica] event 9 (42 bytes)
2025-11-19 22:53:53.439543 I | [replica] event 10 (42 bytes)
2025-11-19 22:53:53.455685 I | [replica] event 11 (42 bytes)
2025-11-19 22:53:53.480662 I | [replica] event 12 (42 bytes)
2025-11-19 22:53:53.497175 I | [replica] event 13 (42 bytes)
2025-11-19 22:53:53.513441 I | [replica] event 14 (42 bytes)
2025-11-19 22:53:53.533515 I | [replica] event 15 (42 bytes)
2025-11-19 22:53:53.552106 I | [replica] event 16 (44 bytes)
\end{lstlisting}

Этот лог подтверждает, что механизм репликации работает в потоковом режиме и не теряет события даже при многократных обновлениях.
