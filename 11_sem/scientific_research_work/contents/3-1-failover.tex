\section{Обеспечение отказоустойчивости}

При размещении двух Raft-кластеров в разных зонах доступности требуется наделить
систему свойством отказоустойчивости. Под этим понимается способность системы
автоматически продолжать обработку запросов на чтение и запись при частичном
или полном отказе одного из кластеров, без ручного вмешательства администратора,
а также с минимальными потерями данных, обусловленными асинхронной природой
межкластерной репликации.

\subsection{Архитектура отказоустойчивости}

В разрабатываемой архитектуре отказоустойчивость реализуется на уровне кластеров
целиком (active/passive), а не отдельных узлов. В каждый момент времени ровно
один кластер находится в активном состоянии и принимает операции записи,
тогда как второй кластер функционирует в пассивном режиме и получает изменения
посредством асинхронной межкластерной репликации (снапшоты и поток WAL-записей).
Основной задачей механизма отказоустойчивости является корректное и безопасное
переключение ролей кластеров при деградации или недоступности активного кластера.

Общая архитектура механизма отказоустойчивости представлена на
рис.~\ref{fig:xdrc-failover-arch}. На схеме показаны два Raft-кластера,
развёрнутые в различных зонах доступности, а также внешний координатор состояния,
используемый для предотвращения сценариев \emph{split-brain}.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{assets/autofailover_architecture.png}
    \caption{Архитектура отказоустойчивости XDCR}
    \label{fig:xdrc-failover-arch}
\end{figure}

Ключевой проблемой при проектировании подобной архитектуры является предотвращение
сценариев \emph{split-brain}, при которых оба кластера одновременно считают себя
активными и принимают операции записи. Для исключения таких ситуаций в систему
вводится внешний координатор состояния, реализованный на базе распределённого
ключ-значение хранилища. В качестве координатора используется etcd, предоставляющий
примитивы аренд (lease) и атомарных транзакций.

В координаторе хранится информация о текущем активном кластере в виде ключа
\texttt{active\_lock}, связанного с арендой с ограниченным временем жизни
(\texttt{TTL}). Наличие данного ключа означает, что кластер, имя которого записано
в значении, обладает правом принимать операции записи. При истечении аренды ключ
удаляется автоматически, что используется в качестве триггера для переключения
ролей кластеров.

Поддержание аренды активного состояния возлагается на лидера Raft внутри активного
кластера. Как показано на рис.~\ref{fig:xdrc-failover-arch}, на лидере запускается
специализированный агент отказоустойчивости, который периодически выполняет
продление аренды (\texttt{KeepAlive}) в координаторе. Тем самым лидер подтверждает
свою работоспособность и доступность связности с другими узлами кластера
(наличие кворума).

\paragraph{Деградация активного кластера.}
В системе реализована политика, при которой активный кластер добровольно
отказывается от своей роли (active $\rightarrow$ passive) при нарушении условий
доступности. В частности, задаётся порог минимального количества «здоровых» узлов
(\texttt{min\_healthy\_nodes}), при снижении ниже которого активный кластер считается
деградировавшим. Агент отказоустойчивости, работающий на лидере, периодически
оценивает доступность кластера на основе информации о подключённых Raft-пирах.
При обнаружении деградации агент намеренно прекращает продление аренды
\texttt{active\_lock}. После истечения аренды ключ автоматически удаляется из
координатора, и право на запись считается освобождённым.

Такой подход соответствует принципу \emph{самодемотирования} активного кластера и
существенно снижает риск ложных переключений, в том числе при сетевых разделениях,
поскольку решение о потере статуса активного принимается самим кластером на основе
локально наблюдаемого состояния, а не внешних эвристик.

\paragraph{Повышение роли пассивного кластера.}
Пассивный кластер непрерывно отслеживает состояние ключа \texttt{active\_lock} в
координаторе. При обнаружении отсутствия ключа (истечение аренды) агент пассивного
кластера инициирует процедуру повышения роли, пытаясь атомарно установить новый
\texttt{active\_lock} с помощью транзакции etcd (операция «создать, если не существует»).
В случае успеха кластер становится новым активным и начинает принимать операции
записи. Одновременно направление межкластерной репликации изменяется: новый активный
кластер становится источником изменений, а ранее активный кластер переходит в режим
пассивного получателя данных.

\paragraph{Контроль согласованности при промоуте.}
Поскольку межкластерная репликация носит асинхронный характер, при аварийном
переключении возможна ситуация, когда пассивный кластер отстаёт от активного по
объёму применённых изменений. Для ограничения возможных потерь данных предусмотрена
опциональная политика \texttt{max\_replication\_lag}, ограничивающая допустимое
отставание пассивного кластера на момент промоута. В случае превышения данного
порога повышение роли не выполняется, и кластер продолжает ожидание, что используется
в экспериментальной части работы.

\paragraph{Версионирование состояния (epoch).}
Для обеспечения корректности при возврате ранее отказавшего активного кластера
используется механизм версионирования состояния (epoch). При каждом успешном захвате
\texttt{active\_lock} значение epoch увеличивается и фиксируется в координаторе.
Эпоха публикуется вместе со статусом кластера и служит маркером актуальности права
на запись. Это гарантирует, что кластер, временно потерявший доступ к координатору,
не сможет самопроизвольно возобновить приём записей после восстановления без явного
подтверждения своей актуальной роли.

\subsection{Ограничения и допущения}

Следует отметить, что предложенный механизм обеспечивает отказоустойчивость на уровне
кластеров, но не реализует прозрачную балансировку запросов записи внутри кластера.
Операции записи корректно обрабатываются только Raft-лидером активного кластера;
узлы-последователи возвращают отказ на запись (\emph{not leader}) и требуют
перенаправления запроса к лидеру. Данное поведение соответствует стандартной модели
использования Raft и упрощает обеспечение согласованности при переключении ролей.
