\section{Архитектура геораспределенной системы}

После изучения ключевых механизмов репликации и принципов обеспечения
согласованности данных возникает необходимость перейти от теоретического
рассмотрения к проектированию архитектуры системы. На данном этапе важным
является определение того, каким образом описанные ранее механизмы могут быть
применены на практике с учётом требований к отказоустойчивости и целостности
данных.

В высокоуровневом представлении поставленная задача сводится к организации
репликации всех данных между различными зонами доступности, каждая из которых
представлена отдельным кластером. При этом архитектура должна обеспечивать
корректную синхронизацию состояний, однозначное определение активного кластера и
возможность безопасного переключения ролей при возникновении отказов.

\subsection{Выскоуровневое представление архитектуры}
% Здесь надо описать выскоуровнево из каких компонентов будет состоять система:
% - два кластера, в каждом по n узлов.
% - Выбор протокола для передачи данных (gRPC).
% - API grpc.
% - Проектирование конфига.
% - Поддержка этого всего в CLI утилите для юзабилити.

Говоря о межкластерной репликации, необходимо ввести несколько терминов:

\begin{itemize}
  \item Кластер называется активным, если все его лидер-узлы
  осуществляют запись, а реплики (ведомые узлы) применяют их вслед за
  лидером.
  \item Обратно, кластер называется пассивным, если все его узлы
  отвечают только на запросы чтения. Следовательно, на пассивный кластер
  накладывается ограничение, которе заключается в том, что лидер-узлы
  не в праве принимать запросы на запись.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{assets/xdrc.png}
    \caption{Высокое представление геораспределенной системы}
    \label{fig:xdrc}
\end{figure}

На рисунке~\ref{fig:xdrc} представлена схема межкластерной геораспределенной
системы, состоящей из активного ($cluster\_a$) и пассивного ($cluster\_b$)
кластеров. В состав активно и пассивного кластеров входят 3 узла
соответственно: $cluster\_a\_node\_1$ (лидер), $cluster\_a\_node\_2$,
$cluster\_a\_node\_3$ у активного и $cluster\_b\_node\_1$ (лидер),
$cluster-b-node-2$, $cluster-b-node-3$ у пассивного. Внутри кластеров имеются
связи, подписанные rpc (remote procedure call), что показывает взаимодействие
узлов по бинарному протоколу удаленного вызова.

Чтобы получить нужные записи из активного кластера, лидеру из пассивного ($cluster\_b\_node\_1$)
необходимо найти мастера из активного кластера и подписаться на его
обновления. Так как лидер $cluster\_a\_node\_1$, подключение происходит к нему.
Если межкластерная репликация запускается впервые, то из мастера
активного кластера сперва вычитается и передается снапшот, который применятеся
на лидер-узле пассивного кластера и далее по rpc протоколу на весь кластер.
Если снапшот был получен ранее, то подписка
происходит за счет прочтения новых записей из журнала WAL, начиная с
последнего номера, называевым Log Sequence Number (LSN), который персистентно
сохраняется отдельно на пассивном кластере.

Таким образом, кластеру-последователю необходимо всегда хранить последнее
состояние, которое ему удалось среплицировать с активного и применить на
собственном. Назовем число такого состояния $last\_applied\_lsn$ Чтобы данные
не терялись при перезапуске системы, $last\_applied\_lsn$ должен также писаться
на диск, аналогично остальным данным, которые пишутся пользователем. 

Протокол, по которому будет идти репликационный поток между кластерами -- gRPC.
Выбор именно этого протокола обусловлен быстротой и эффективностью, а также
возможностью создать единый поток, по которому будут ходить репликационные
пакеты. Это решение эффективнее по сравнению с наивной реализацией, где на
каждую запись вызывался бы новый запрос.

\subsection{API межкластерного взаимодействия}

Как было сказано выше, для сетевого взаимодействия между класетрами используется проткол gRPC \cite{grpc}.

Было выненсено два метода: $GetSnapshots$ и $GetEvents$. Первый метод нужен для
инициализации пассивного кластера, когда он подключатеся к активноому кластеру
впервые. Как было сказано ранее, вычитывать снимок эффективнее, чем проигрывать
журнал предзаписи. Конечно, его польза значима только в случае, когда активный
кластер хранит достаточное количество данных, которое проигрывалось бы из WAL
дольше, нежели единым снимком.

Для языка Go был написан protobuf (protocol buffer) \cite{protobuf} файл, представленный на листинге~\ref{protobuf}, который содержит интерфейс взаимодействия кластеров.

\listing[
    caption={Описание интерфейса общения кластеров на языке Protobuf},
    label=protobuf
]{assets/interface.proto}

Метод $GetSnapshots$ не содержит аргументов, так как он вычитывает
снапшот-файл, и отдает поток репликационных событий. Выбор потока обусловлен
улучшением производительности системы.

Репликационное событие имеет тип $Event$. Оно содержит следующие поля:

\begin{itemize}
  \item $lsn$ (тип uint64) -- log sequence number текущего события;
  \item $id$ (тип string) -- ключ хранилища;
  \item $data$ (тип string) -- непосредственное значение ключа хранилища.
\end{itemize}

Метод $GetEvents$ принимает в качестве аргумента $State$, который включает в
себя номер (lsn) последнего примененного на пассивном кластере события. Аналогично получению
снапшотов, $GetEvents$ возвращает поток репликационных событий.

\subsection{API взаимодейстия пользователя с системой}

Программный интерфейс приложения дает возможность пользователю взаимодействовать с кластерами.
Как было сказано ранее, активный кластер примает запросы на запись и чтение, а пассивный только на чтение.
Полный HTTP API описан в таблице~\ref{tab:http-api}.

\begin{table}[H]
    \centering
    \small
    \caption{HTTP API геораспределённого KV-хранилища}
    \begin{tabularx}{\textwidth}{|X|X|X|X|}
    \hline
    \textbf{Метод} & \textbf{URL} & \textbf{Доступный кластер} & \textbf{Описание} \\ \hline
    GET    & /key/\{key\} 
           & Любой узел любого состояния кластера
           & Получение значения по ключу. На пассивном кластере доступно только чтение. \\ \hline
    POST   & /key/\{key\} 
           & Лидер-узел активного кластера 
           & Создание или обновление значения ключа. Доступно только на активном кластере. \\ \hline
    DELETE & /key/\{key\} 
           & Лидер-узел активного кластера 
           & Удаление ключа. Доступно только на активном кластере. \\ \hline
    GET    & /keys?limit=\{n\}
           & Любой узел любого состояния кластера
           & Возвращает отсортированный список всех записей в хранилище (key–value), 
             ограниченный параметром \texttt{limit}. \\ \hline 
    GET    & /status 
           & Любой узел любого состояния кластера
           & Получение текущего состояния кластера: лидер, последователи, 
             идентификатор текущего узла. \\ \hline
    POST   & /join 
           & Внутренний вызов для присоединения узла ко кворуму кластера
           & Добавление узла в кластер. В теле запроса передаются \texttt{id} и \texttt{addr}
             нового узла. \\ \hline
    \end{tabularx}
    \label{tab:http-api}
\end{table}

По большей части HTTP API представляет собой CRUD операции над кластером, а также операции по получению статуса кластера и возможность присоединения узла ко кворуму внутри кластера.

\subsection{Выбор библиотек}

Для реализации внутренних механизмов консенсуса и межкластерной репликации
использовались библиотеки, обеспечивающие формальную корректность, устойчивость
к сбоям и совместимость с архитектурными требованиями проекта.

В основе работы каждого кластера лежит библиотека \texttt{hashicorp/raft},
представляющая промышленную реализацию алгоритма Raft. Её использование
обусловлено следующими факторами:

\begin{itemize}
  \item наличие полного соответствия спецификации Raft (лидерство, репликация лога,
  снапшоты, восстановление);
  \item предоставление чётко определённых интерфейсов \texttt{FSM}, \texttt{LogStore} и
  \texttt{SnapshotStore};
  \item устойчивая эксплуатация в больших проектах (Consul, Nomad), что подтверждает её
  корректность и зрелость модели.
\end{itemize}

Выбор готовой реализации исключает необходимость самостоятельной разработки
алгоритма консенсуса и гарантирует предсказуемое поведение кластера.

Для хранения журнала предзаписи используется \texttt{hashicorp/raft-wal},
реализующая интерфейс \texttt{raft.LogStore}. Данная библиотека обеспечивает: сегментированное хранение WAL-файлов, надёжное восстановление состояния после сбоя, последовательный доступ к записям, необходимый для реализации механизма межкластерной репликации.

Совместимость с исходной реализацией Raft позволяет использовать WAL
одновременно как механизм персистентности данных внутри кластера и как источник
репликационных событий.

\subsection{Проектирование конфига}

Для корректного развёртывания геораспределённой системы требуется единый формат
конфигурации, определяющий структуру кластера, параметры отдельных узлов и
сведения о межкластерных связях. Конфигурационный файл описывается в формате
YAML и включает как локальные параметры кластера (адреса узлов, директории
хранения), так и информацию, необходимую для организации межкластерной
репликации.

Конфигурация для активного и пассивного кластеров имеет одинаковую структуру,
различаясь только значениями полей \texttt{cluster\_status} и
\texttt{follow\_list}. Основные элементы конфигурации перечислены ниже.

\begin{itemize}
\item \textit{data\_dir} — директория для хранения данных узлов кластера.
\item \textit{bin\_path} — путь к каталогу, содержащему исполняемые файлы узлов.  
      Используется вспомогательными инструментами CLI для запуска процессов.

\item \textit{cluster} — список, содержащий описание всех узлов одного кластера.
\item \textit{cluster\_name} — логическое имя кластера, позволяющее различать активный и пассивный кластеры в конфигурациях межкластерной репликации.
\item \textit{cluster\_status} — режим работы кластера: active (кластер принимает операции записи и распространяет их среди своих узлов) и passive (кластер доступен только для чтения и получает изменения от активного кластера).

\item \textit{leader} — обозначение узла, который должен быть инициирован как лидер при первичном запуске.  
      Используется только в сценарии bootstrap’а, после чего лидерство передаётся алгоритму Raft.
\item \textit{follow\_list} — список gRPC адресов серверов сосднего кластера.
\end{itemize}

Секция \texttt{cluster} содержит перечень узлов и их адресов. Каждый узел описывается следующим набором параметров:

\begin{itemize}
\item \textit{alias} — уникальный идентификатор узла внутри кластера.
\item \textit{http\_address} — адрес, на котором узел обслуживает HTTP API (CRUD-операции пользователя, статус кластера).

\item \textit{rpc\_address} — адрес, используемый механизмом консенсуса Raft для обмена служебными сообщениями между узлами текущего кластера.

\item \textit{grpc\_address} — адрес, по которому узел предоставляет gRPC API для межкластерной репликации.  
      Именно по этому адресу пассивный кластер подключается к активному.
\end{itemize}

Помимо описания локального кластера, конфигурация содержит два параметра, относящихся к межкластерному взаимодействию:

\begin{itemize}
\item \textbf{follow\_list} — список gRPC-адресов всех узлов удалённого (противоположного) кластера. Пассивный кластер использует эти адреса для выбора лидера активного кластера по алгоритму round-robin и установления потока репликации. На активном кластере \texttt{follow\_list} обычно указывает на пассивный, но используется только вспомогательными инструментами.
    \item \textbf{cluster\_status}: принимает значения \texttt{active} и \texttt{passive}
\end{itemize}
