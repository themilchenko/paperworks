\section{Постквантовая криптография}

Постквантовая криптография — это область криптографии, в которой разрабатываются 
криптографические алгоритмы, устойчивые к противнику, обладающему квантовым компьютером. 
Все современные асимметричные алгоритмы (RSA, ECC, DH, DSA) становятся уязвимыми 
в квантовой модели вычислений. Они основаны на задачах факторизации и дискретного 
логарифмирования, которые эффективно решаются на квантовых компьютерах с помощью 
алгоритма Шора. Ранее математики и криптографы полагались на трудность этих задач 
числовой теории, однако теперь возникает необходимость искать новые математические 
проблемы, которые не поддаются квантовым атакам.

Симметричные алгоритмы и хеш-функции считаются сравнительно безопасными в 
постквантовом мире. Алгоритм Гровера ускоряет атакующую переборную операцию лишь 
до квадратного корня от сложности cверху $\mathcal{O}(\sqrt{N})$ 
\cite{NayaPlasencia2016QuantumPeriod}. Однако большинство симметричных систем 
можно сделать вновь стойкими просто удвоением длины ключа.

Все существующие асимметричные алгоритмы основаны на математических задачах, 
которые исследуются сотни лет. Их слабое место состоит в том, что квантовые 
компьютеры эффективно выполняют параллельные вычисления, если необходимо получить 
единственный итоговый результат. Благодаря суперпозиции кубитов квантовая машина 
может параллелизовать вычисления над всей областью поиска и затем выполнить измерение.

Чтобы исключить возможность использования квантового параллелизма, перспективные 
постквантовые алгоритмы должны основываться на задачах, в которых требуется не один, 
а множество результирующих значений, либо задачах, чья структура плохо поддаётся 
квантовым преобразованиям.

В настоящее время большинство постквантовых алгоритмов относится к шести основным 
семействам. Каждое семейство использует собственный класс математических задач, которые 
остаются трудными даже для квантовых компьютеров. Эти задачи лягут в основу следующего 
поколения асимметричной криптографии. 

NIST начал процесс стандартизации постквантовой криптографии в 2016 году. Цель процесса — 
выбор новых схем цифровых подписей и алгоритмов шифрования/обмена ключами, устойчивых 
к атакам квантовых компьютеров.

\subsection{Криптография на решетках}

Криптография на решётках основана на трудных задачах теории решёток. 
В таких схемах безопасность опирается на семейство задач на решётках, причём 
ключевой особенностью является то, что стойкость основана на задачах в худшем случае. 
Большинство других криптосистем опираются на сложность в среднем случае 
\cite{Ajtai1997}.

Решётка представляет собой регулярно расположенную бесконечную сетку точек. 
Вектор можно рассматривать как точку решётки, то есть набор координат. 
Начало координат соответствует вектору, все координаты которого равны нулю. 
Вектор называют \emph{коротким}, если он расположен близко к началу координат, 
и \emph{длинным}, если он находится далеко от него.  

\textbf{Базисом} решётки называется небольшой набор векторов, линейные комбинации 
которых порождают всю решётку. Для $n$-мерной решётки выбирают $n$ векторов так, 
чтобы никакая другая точка решётки не лежала на прямой между произвольным базисным 
вектором и началом координат.

Для одной и той же решётки может существовать множество базисов. 
Базис называется \emph{коротким}, если он состоит из коротких векторов, и 
\emph{длинным}, если векторы базиса длинные. На основе решёток сформулированы 
несколько фундаментальных трудных задач:

\begin{itemize}
  \item \textbf{Задача кратчайшего вектора (SVP)}: дан длинный базис решётки $L$. 
  Требуется найти точку решётки $L$, расположенную к началу координат 
  как можно ближе.

  \item \textbf{Задача короткого базиса (SBP)}: дан длинный базис решётки $L$. 
  Необходимо найти короткий базис той же решётки.

  \item \textbf{Задача ближайшего вектора (CVP)}: дан длинный базис решётки $L$ 
  и точка $P$ в пространстве. Требуется найти точку решётки, наиболее близкую к $P$.

  \item \textbf{Задача короткого целочисленного решения (SIS)}:  
  даны $m$ векторов размерности $n$, $v_i \in \mathbb{Z}_q^n$.  
  Требуется найти вектор коэффициентов $y \in \mathbb{Z}_q^m$ из малых значений 
  (например, $y_i \in \{0,1\}$), такой что линейная комбинация 
  $\sum_i y_i v_i = 0$.
\end{itemize}

Во «в малых размерностях» эти задачи выглядят элементарными. Однако в криптографии 
решётки имеют очень большие размерности, и при заданном длинном базисе указанные 
задачи становятся крайне сложными. Если бы противнику был известен короткий базис, 
они были бы легко решаемы, но использование только длинного базиса делает их 
хорошими кандидатами для построения криптосхем.

Изучение решёток начинается ещё в XIX веке, и накопленные результаты дают хорошее 
понимание того, что именно можно и нельзя эффективно делать с такими структурами. 
Это укрепляет уверенность в надёжности решёточных задач как основы асимметричных 
алгоритмов. Основной класс атак использует алгоритмы редукции решёток, например LLL: 
это полиномиальный алгоритм, позволяющий по заданному длинному базису получить 
относительно короткий (но не оптимальный) базис.

В 1997 году Аджтай предложил криптосистему, основанную на задаче кратчайшего 
вектора \cite{Ajtai1997}. Уже в 1998 году Нгуен и Штерн показали криптоанализ 
криптосистемы Аджтая--Дворка \cite{NguyenStern1998AjtaiDwork}.  
Алгоритм Голдрейха--Голдвассера--Халеви \cite{Goldreich1997Lattice}, основанный на 
задаче ближайшего вектора (CVP), был впоследствии взломан Нгуеном \cite{Nguyen1999GGH}. 

Схема NTRU была предложена в 1996 году и затем формально описана в 
\cite{Hoffstein1998NTRU}. За прошедшие годы она неоднократно модифицировалась 
и усиливалась, и современный вариант NTRU рассматривается как один из финальных 
кандидатов в процессе стандартизации NIST.

\subsubsection{Схема шифрования GGH}

В схеме шифрования GGH приватный ключ получателя представляет собой \emph{короткий базис} решётки, 
а публичный ключ — \emph{длинный базис}. Число измерений решётки выбирается равным длине сообщения 
в битах. Отправитель использует «плохой» (длинный) базис, чтобы найти точку в пространстве решётки, 
расположенную близко к некоторой точке решётки, которая и кодирует сообщение. Обладая коротким базисом, 
получатель может эффективно «спроецировать» полученную точку на ближайшую решёточную точку и тем самым 
восстановить исходное сообщение.

Для противника задача расшифрования трудна, поскольку в его распоряжении только длинный базис. 
Схема изначально рассматривалась как потенциально стойкая к квантовым атакам, однако была взломана 
на классических компьютерах из-за определённых уязвимостей: при накоплении большого числа 
зашифрованных сообщений становилось возможно восстановить часть информации о закрытых текстах. 

Тем не менее идеи, заложенные в GGH, с учётом уменьшения уязвимостей и модификаций конструкций, 
нашли отражение во многих современных кандидатах на постквантовые криптосистемы.

\subsubsection{Обучение с ошибками}

Learning With Errors (обучение с ошибками, LWE) — это подмножество
решёточной криптографии. Оно основано на новой 
функции с потайным входом (trapdoor function), которую легко вычислить,
но трудно инвертировать.

В исходной задаче система уравнений имеет вид \(AX = B\), и если заданы \(A\) и \(B\),
то найти \(X\) несложно — достаточно применить метод Гаусса \cite{RegevLWE}.
Однако если к произведению \(AX\) добавить случайную ошибку \(e\), так что получается $AX + e = B$
то, обладая только \(A\) и \(B\), найти \(X\) становится крайне трудной математической задачей.
Эта трудность и используется как криптографическая опора.

Существуют модификации LWE: обучение с ошибками в кольце Ring-LWE и обучение с ошибками по модулю Module-LWE, которые уменьшают размеры ключей, сохраняя стойкость схемы.

Система уравнений \(AX = B\) может иметь больше уравнений, чем переменных, но она
конструируется так, чтобы быть разрешимой. Приватный ключ — это вектор переменных \(X\),
а публичный ключ — матрицы \(A\) и \(B\).

Для шифрования сообщения отправитель выбирает случайное подмножество строк матрицы,
складывает их и затем: добавляет большую ошибку, чтобы закодировать бит \(1\) и
добавляет маленькую ошибку, чтобы закодировать бит \(0\). Получатель, зная приватный ключ, легко определяет, была добавлена большая или маленькая ошибка,
и таким образом восстанавливает сообщение. Для злоумышленника же задача определения \(X\)
по \(A\) и \(B\) остаётся вычислительно сложной.

Сегодня 27 из 69 алгоритмов, поданных на стандартизацию NIST, основаны на LWE-подобных задачах.
Компания Google уже внедрила LWE в Chrome для обеспечения постквантовой безопасности.

\subsection{Цифровые подписи на основе хеширования}

Цифровые подписи на основе хеширования представляют собой альтернативу современным
схемам цифровой подписи, использующим асимметричные алгоритмы вроде RSA. Их
криптографическая стойкость опирается на два ключевых свойства хеш-функций:
стойкость к коллизиям и стойкость к прообразу.

Стойкость к прообразу означает, что по заданному выходному значению \(y\)
затруднительно найти вход \(x\), такой что \(H(x) = y\).  
Слабая стойкость к коллизиям означает, что по заданному сообщению \(m_1\)
трудно найти другое сообщение \(m_2\) с тем же значением хеша: \(H(m_1) = H(m_2)\).  
Сильная стойкость к коллизиям означает сложность поиска двух сообщений
\(m_1\) и \(m_2\), удовлетворяющих \(H(m_1) = H(m_2)\).
Замечание: атака на сильную стойкость упрощается благодаря парадоксу дней рождения.
Если слабая стойкость к коллизиям и стойкость к прообразу требуют порядка \(O(2^{n-1})\)
операций (где \(n\) — длина выходного значения хеша), то сильная стойкость требует
лишь \(O(2^{n/2})\).

При условии использования хорошей криптографической хеш-функции задача нахождения
коллизий или прообразов остаётся крайне сложной. Создание квантовых алгоритмов,
способных эффективно решать такие задачи, вероятно, также будет крайне сложным или
невозможным. Поэтому цифровые подписи на основе хеширования рассматриваются как
надёжный инструмент аутентификации в постквантовом мире.
Однако у таких схем есть важный недостаток: каждая одноразовая подпись может
быть использована только один раз.

Из 69 схем, представленных на конкурс NIST, две относятся к криптографии на основе
хеширования. Лэмпорт предложил первую такую схему цифровой подписи в 1979 году
\cite{Lamport1979}.  
Винтерниц разработал одноразовую схему подписи, значительно более эффективную,
чем схема Лэмпорта: она использует более короткие ключи и подписи.
Меркл предложил схему, объединяющую идею Винтерница с бинарными хеш-деревьями,
которая получила название "Подпись Меркла".  
Схема SPHINCS+, являющаяся одним из кандидатов NIST, использует комбинацию
одноразовой подписи Винтерница (WOTS+) и хеш-деревьев Меркла в конструкции
Forest of Random Subsets.

\subsubsection{Схема цифровой подписи Лэмпорта}

Схема цифровой подписи Лэмпорта представляет собой одноразовую схему подписи.
Для её работы требуется криптографически стойкая хеш-функция.  
Если уровень безопасности равен \(b\), то необходима хеш-функция с длиной выхода \(2b\).
Например, при необходимости обеспечить 128-битную безопасность можно использовать
любую хеш-функцию с 256-битным выходом.

Закрытый ключ генерируется с помощью генератора случайных чисел:
создаётся 256 пар случайных чисел, каждое длиной 256 бит.  
Эти значения и являются закрытым ключом.  
Размер закрытого ключа равен \(8b^2\).

Открытый ключ состоит из 512 хешей всех случайных чисел, сгенерированных для закрытого ключа.  
Размер открытого ключа также составляет \(8b^2\).  
Эти значения публикуются пользователем, который собирается подписывать документы.
Алгоритм подписи \cite{Bernstein2009} требует вычисления хеша сообщения. Поскольку хеш имеет длину 256 бит,
для каждого бита (в зависимости от его значения 0 или 1) выбирается одно число
из соответствующей пары закрытого ключа.  
Таким образом, цифровая подпись представляет собой последовательность из 256 чисел,
которая публикуется вместе с сообщением.

Проверяющий вычисляет хеш полученного сообщения. Затем для каждого бита хеша выбирает
соответствующий хеш из открытого ключа. После этого проверяющий хеширует каждое число
из подписи и убеждается, что полученные значения совпадают с выбранными элементами
открытого ключа.
Так как раскрытие отдельных элементов закрытого ключа происходит во время подписи,
данная схема является одноразовой: после одного использования пара ключей
(закрытый и открытый) должна быть уничтожена.

\subsection{Криптография на основе кодов, исправляющих ошибки}

Криптография на основе кодов опирается на коды, исправляющих ошибки \cite{Bernstein2009}. 
Компьютерные учёные изучают их уже более 40 лет. Коды коррекции ошибок широко 
используются в системах связи для исправления ошибок передачи. Чтобы отправить сообщение, 
текст пропускают через код коррекции ошибок, после чего в полученный результат 
случайным образом добавляют несколько ошибок и отправляют его.

Одним из наиболее известных криптосистем данного типа является алгоритм МакЭлиса \cite{BiswasNandi2008McEliece}. 
Он использует линейные коды коррекции ошибок (матричное умножение). 
Получатель обладает хорошим кодом коррекции ошибок, который является закрытым ключом. 
Этот код умножается на две маскирующие матрицы, что приводит к получению «плохого» 
кода коррекции ошибок — открытого ключа. Открытый ключ публикуется. 
Отправитель пропускает открытый текст через плохой код коррекции ошибок, 
а затем добавляет некоторое число ошибок согласно параметрам избыточности. 
Полученный результат является итоговым шифртекстом. Получатель использует свой хороший 
код коррекции ошибок для расшифровки.

Алгоритм МакЭлиса был предложен в 1978 году \cite{McEliece1978}, и до настоящего времени 
никаких уязвимостей в нём обнаружено не было. Основная причина отсутствия широкого 
практического применения — очень большой размер открытого ключа, который значительно 
превышает размеры открытых ключей в асимметричных схемах вроде RSA.

В соревновании NIST из 69 представленных алгоритмов 21 относится к криптосистемам 
на основе кодов, исправляющих ошибки.

\subsection{Некоммутативная криптография}

Некоммутативная криптография опирается на некоммутативные группы 
(где $A + B \ne B + A$). Простым примером служит кубик Рубика: 
последовательность ходов представляет собой набор действий, применяемых 
к кубику \cite{Schmeh2020Book}. Операция сложения соответствует 
конкатенации последовательностей ходов. Такое сложение является 
некоммутативным. 

Обратный ход — это действие, противоположное предыдущему ходу, 
что позволяет устранять пары действий и записывать последовательность 
без них. Отрицание последовательности ходов состоит из всех 
противоположных ходов, записанных в обратном порядке. Таким образом, 
вся последовательность может быть обращена. Отрицание последовательности $A$ 
обозначается как $-A$.

\textbf{Задача сопряжённости}: даны две последовательности ходов $A$ и $B$. 
Требуется найти $X$ такое, что $X + A - X = B$.

Поскольку операция сложения некоммутативна, задача является очень трудной. 
Односторонняя функция легко вычисляется, но её обращение крайне затруднено.

\subsubsection{Протокол обмена ключами Стикеля}

Поскольку протокол Диффи–Хеллмана не является квантово-устойчивым, 
в качестве альтернативы используется протокол обмена ключами Стикеля. 
Он считается квантово-устойчивым и основан на некоммутативной 
криптографии \cite{Shpilrain2008Stickel}.

В протоколе Стикеля \cite{Stickel2005KeyExchange} задаются две 
последовательности ходов $A$ и $B$. У Алисы и Боба также есть два 
натуральных числа, которые служат их закрытыми ключами. Пусть у Алисы 
это $n$ и $m$, а у Боба — $r$ и $s$.
Алиса генерирует открытый ключ $PK_a = mA + nB$
и отправляет его Бобу. Боб генерирует открытый ключ $PK_b = rA + sB$
и отправляет его Алисе. Затем Алиса вычисляет
$K_a = mA + PK_b + nB = (m + r)A + (s + n)B$,
а Боб вычисляет $K_b = rA + PK_a + sB = (r + m)A + (n + s)B$.

Полученные ключи совпадают, и противник не может их вычислить 
из-за некоммутативности группы: для перехватчика решение задачи 
сопряжённости остаётся вычислительно трудным.

В конкурсе NIST только один алгоритм из 69 был некоммутативным 
криптосистемным подходом. Он был взломан, поэтому ни один 
некоммутативный криптосистемный алгоритм не будет стандартизирован.

\subsection{Многомерная криптография}

Многомерная криптография основана на трудной математической
задаче решения системы многочленов от нескольких переменных.
Все многомерные криптосистемы опираются на многомерное квадратичное
отображение \cite{Ding2009MultivariatePKC}. Квадратичное отображение
берёт последовательность $x = (x_1, \ldots, x_n) \in \mathbb{F}_q^n$
и возвращает вектор $y = (p_1(x), \ldots, p_m(x)) \in \mathbb{F}_q^m$,
где $p_i(x)$ — квадратичные многочлены от нескольких переменных
для $i = 1, \ldots, m$, а коэффициенты многочленов принадлежат $\mathbb{F}_q$.
Такое отображение называется многомерным квадратичным отображением
$P$ с $m$ компонентами и $n$ переменными.

\textbf{Задача MQ}: дано квадратичное отображение
$P : \mathbb{F}_q^n \to \mathbb{F}_q^m$
и целевой вектор $t \in \mathbb{F}_q^m$. Требуется найти $s$ такое, что
$P(s) = t$. Значение $s$ не является уникальным, так как отображение
$P$ не является инъективным.

Эта задача считается трудной даже для квантовых компьютеров.
Существуют методы, такие как базисы Грёбнера, позволяющие решать
данную задачу. В последние годы для решения MQ-задачи используются
усовершенствованные алгоритмы, похожие на базисы Грёбнера:
алгоритмы F4/F5 и XL \cite{Courtois2000Overdefined}.

Основная область применения MQ — цифровые подписи.
Наиболее известной схемой цифровой подписи в многомерной
криптографии является схема «Масло и Уксус» (Oil and Vinegar).
Rainbow — схема цифровой подписи, созданная на основе
несбалансированной версии Oil and Vinegar, и является финалистом
конкурса NIST.

\subsubsection{Схема «Масло и уксус»}

Схема цифровой подписи основана на сложной математической MQ-задаче. 
Она использует два типа переменных — «масло» и «уксус» (oil и vinegar). 
Выбирается случайное квадратичное отображение 
$P : \mathbb{F}_q^n \to \mathbb{F}_q^m$. 
Число переменных равно $n$, а число квадратичных многочленов — $m$.
В отображении выделяются $m$ переменных типа «масло» и $n - m$ переменных типа «уксус» \cite{KipnisShamir1999UOV}.

Ключевая идея схемы заключается в том, что квадратичные члены многочленов
могут содержать только комбинации вида «уксус–уксус» и «масло–уксус». 
Комбинаций вида «масло–масло» нет. 
После построения многочлены маскируются обратимым линейным преобразованием, 
чтобы противник не мог различить типы переменных.

Алиса, желая подписать документ, публикует композицию 
$R = P \circ T$, где $T$ — обратимое линейное преобразование, 
а сами $P$ и $T$ хранятся как закрытый ключ. 
Для подписи документа $d$ она вычисляет хэш $y = H(d)$, 
затем вычисляет $s' = T^{-1}(y)$, после чего находит $s$ такое, что 
$s = P^{-1}(s')$ — это и есть подпись. 
Получатель использует открытый ключ $R$, вычисляет $R(s)$ 
и сравнивает результат с хэшем сообщения.

Для Алисы нахождение $s$ просто, поскольку она знает, какие переменные являются 
«маслом», а какие — «уксусом». Она выбирает случайные значения уксусных переменных 
и подставляет их. Поскольку квадратичные члены зависят только от уксуса, 
получается линейная система уравнений по масляным переменным. 
Число масляных переменных равно $m$, и уравнений также $m$, 
поэтому система решается методом Гаусса.  
Для противника же, не знающего разделения переменных, обратная задача сводится 
к NP-трудной MQ-проблеме.

Изначальная схема «Масло и уксус» \cite{Patarin1997OV} имела одинаковое число 
масляных и уксусных переменных и была разобрана Кипнисом и Шамиром 
\cite{KipnisShamir1998OVCryptanalysis}. 
Впоследствии была предложена новая схема — Unbalanced Oil and Vinegar (UOV) 
\cite{KipnisShamir1999UOV}, в которой числа переменных разных типов отличаются. 
Подбор корректных параметров $n$ и $m$ позволяет повысить устойчивость схемы.

Схема цифровой подписи Rainbow представляет собой многослойную систему, 
построенную на нескольких уровнях UOV \cite{DingSchmidt2005Rainbow}. 
Это усложняет прямые атаки на MQ-задачу, 
но одновременно увеличивает уязвимость к атаке MinRank.

В задаче \textbf{MinRank} даны $k$ матриц $M_i$ размером $n \times m$ и целевой ранг $r$. 
Необходимо найти коэффициенты $y$, такие что их линейная комбинация 
имеет ранг не выше $r$.  
Увеличение числа слоёв Rainbow повышает уязвимость к MinRank, поэтому 
в стандартизируемой версии используется оптимизация из двух слоёв. 
Недавно Бёлленс \cite{BeullensUOVRainbow} предложил новую пересекающую атаку, 
уменьшающую предполагаемый уровень безопасности Rainbow на несколько бит.

\subsection{Криптосистемы на основе изогений}

Криптосистемы на основе изогений используют свойства эллиптических кривых. 
Эллиптическая кривая определяется как множество решений полиномиального уравнения 
двух переменных. Обычно кривые задаются уравнением вида 
$y^2 = x^3 + ax + b$, где выполняется условие $4a^3 + 27b^2 \neq 0$. 
Это условие гарантирует отсутствие особых точек.

Эллиптические кривые обладают следующим свойством: если соединить две точки 
кривой прямой, то эта прямая пересечёт кривую в третьей точке. 
Отражение этой точки относительно оси $X$ даёт результат операции сложения 
на эллиптической кривой. Удвоение точки осуществляется аналогично, только вместо 
прямой используется касательная в этой точке. Скалярное умножение 
$[n]P$ означает сложение точки $P$ самой с собой $n$ раз.

Алгоритм Диффи—Хеллмана на эллиптических кривых (ECDH) стал первым 
алгоритмом обмена ключами на эллиптических кривых \cite{HaakegaardLang2015ECDH}. 
В нём фиксируется эллиптическая кривая над полем по модулю простого числа $p$ 
и точка $P$ со специальными свойствами. Алиса и Боб выбирают случайные числа 
$n_a$ и $n_b$ как свои закрытые ключи, вычисляют 
$P_a = [n_a]P$ и $P_b = [n_b]P$ и обмениваются ими. 
Далее Алиса вычисляет $[n_a]P_b = [n_an_b]P$, а Боб — $[n_b]P_a = [n_an_b]P$. 
Полученная точка служит общим секретом.

Безопасность ECDH основана на сложности задачи дискретного логарифмирования 
на эллиптических кривых. Однако эта задача решается квантовым алгоритмом Шора 
за полиномиальное время, поэтому ECDH считается небезопасным при наличии 
квантовых компьютеров.

После этого началось интенсивное исследование эллиптических кривых и их 
морфизмов, что привело к появлению криптосистем на основе изогений. 

\subsubsection{Схема Диффи–Хеллмана на суперсингулярных изогениях (SIDH)}

Изогения — это гомоморфное рациональное отображение между двумя эллиптическими
кривыми $\Phi : E_0 \rightarrow E_1$. В SIDH Алиса и Боб генерируют приватные изогении,
которые и являются их закрытыми ключами. Они применяют свои изогении к общей
эллиптической кривой $E$, после чего обмениваются полученными результатами.
Получив новую кривую, другая сторона применяет свою приватную изогению, чтобы
получить ещё одну кривую. В отличие от ECDH, конечные кривые у Алисы и Боба 
не обязаны быть одной и той же кривой, однако они имеют одинаковую структуру:
кривые являются изоморфными. Для эллиптической кривой можно вычислить 
величину $j$-инварианта, которая одинакова для изоморфных кривых. Поэтому 
$j$-инвариант вычисляется Алиской и Бобом независимо и служит общим секретом.
Торсионная подгруппа $E[n]$ эллиптической кривой — это множество всех точек $P\in E$,
удовлетворяющих $[n]P = 0$, где 0 — аддитивная единица на $E$.

SIDH был предложен в 2011 году \cite{JaoDeFeo2011SIDH}. В схеме фиксируется
суперингулярная эллиптическая кривая $E$, определённая над полем $\mathbb{F}_q$, где
$q = p^2$ и $p = 2^a 3^b - 1$. Алиса и Боб совместно используют кривую $E$, а также
базисы $P_a, Q_a$ для $E[2^a]$ и $P_b, Q_b$ для $E[3^b]$ 
(базис — это набор точек, линейные комбинации которых порождают всю группу точек).

Алиса и Боб выбирают случайные числа $r_A$ и $r_B$ из диапазонов 
$0 \le r_A < 2^{a-1}$ и $0 \le r_B < 3^{b-1}$ соответственно. 
Алиса вычисляет свою изогению $\Phi_A$, ядро которой задаётся точкой 
$R_A = P_A + [r_A]Q_A$. Затем она публикует $E_A = \Phi_A(E)$,
а также $\Phi_A(P_b)$ и $\Phi_A(Q_b)$ как свой открытый ключ.
Эти данные позволяют Бобу вычислить свою вторую изогению $\Psi_B$.
Боб строит $\Psi_B$ с ядром, порождённым точкой 
$\Phi_A(P_b) + [r_B]\Phi_A(Q_b)$, и затем вычисляет 
$j$-инвариант кривой $\Psi_B(E_A)$. Алиса выполняет аналогичную процедуру,
в результате чего обе стороны получают один и тот же общий секрет.

Безопасность схемы основана на сложности задачи $l^e$-изогении. Эта задача 
сформулирована так: даны две эллиптические кривые $E_1$ и $E_2$, между которыми
существует изогения $\Phi$ с ядром размера $l^e$; необходимо определить ядро $\Phi$.
В SIDH это задачи $2^a$-изогении и $3^b$-изогении. Математики исследуют 
эти задачи уже более 20 лет, и для больших степеней $e$ и любого натурального $l$
не найдено эффективных атак.

Механизм инкапсуляции ключей SIKE (Supersingular Isogeny-Based Key Encapsulation)
основан на SIDH и входил в список альтернативных кандидатов третьего раунда NIST
на стандартизацию схем инкапсуляции ключей.
